{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "import umap\n",
    "import joblib\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import KernelPCA, PCA\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = joblib.load('./mpdocvectorsdict100.x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y = joblib.load('./all_vote_universal_credit_labels.y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = list(test_y.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303\n",
      "267\n"
     ]
    }
   ],
   "source": [
    "one_count = 0\n",
    "zero_count = 0\n",
    "for i in y:\n",
    "    if i == 0:\n",
    "        zero_count += 1\n",
    "    else:\n",
    "        one_count += 1\n",
    "print(zero_count)\n",
    "print(one_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5        0.43103448 0.48275862 0.42105263 0.38596491 0.40350877\n",
      " 0.50877193 0.41071429 0.51785714 0.58928571]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.46509484919194544"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "LogisticRegression(C=1)\n",
    "scores = cross_val_score(reg, x, y, cv=10)\n",
    "print(scores)\n",
    "scores.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from collections import Counter\n",
    "from scipy import sparse\n",
    "import collections\n",
    "import re\n",
    "import joblib\n",
    "\n",
    "# Custom Transformer that extracts columns passed as argument to its constructor\n",
    "class TfidfFeatureExtractor(BaseEstimator, TransformerMixin):\n",
    "    # Class Constructor\n",
    "    def __init__(self):\n",
    "        self._feature_names = None\n",
    "        self._useful_hashtags = None\n",
    "\n",
    "    def vectoriser(self, features):\n",
    "        dict_vectorizer = DictVectorizer()\n",
    "        hashtags = list(features.values())\n",
    "        sparse_adjacency_matrix = dict_vectorizer.fit_transform(hashtags)\n",
    "        hashtags_df = pd.DataFrame(sparse_adjacency_matrix.todense(), index=features.keys()\n",
    "                                   , columns=dict_vectorizer.get_feature_names())\n",
    "        sparse_csr_matrix = sparse.csr_matrix(hashtags_df.values)\n",
    "        return sparse_csr_matrix\n",
    "    \n",
    "    def features_transformer(self, X):\n",
    "        new_features = collections.OrderedDict()\n",
    "        features = X\n",
    "        for mp_handle, feature_dict in features.items():\n",
    "            new_feature_dict = {}\n",
    "            for feature in self._useful_hashtags:\n",
    "                if feature in feature_dict:\n",
    "                    new_feature_dict[feature] = feature_dict[feature]\n",
    "                else:\n",
    "                    new_feature_dict[feature] = 0\n",
    "            new_features[mp_handle] = new_feature_dict\n",
    "        return new_features\n",
    "\n",
    "    def hashtag_deleter(self, X):\n",
    "        new_features = collections.OrderedDict()\n",
    "        features = X\n",
    "        for mp_handle, feature_dict in features.items():\n",
    "            new_feature_dict = {}\n",
    "            for feature in feature_dict:\n",
    "                if feature in self._useful_hashtags:\n",
    "                    new_feature_dict[feature] = feature_dict[feature]\n",
    "            new_features[mp_handle] = new_feature_dict\n",
    "        return new_features\n",
    "\n",
    "    def mp_deleter(self, X, y):\n",
    "        features = X\n",
    "        handles = list(features.keys())\n",
    "        for mp_handle in handles:\n",
    "            if mp_handle not in y.keys():\n",
    "                del features[mp_handle]\n",
    "        return features\n",
    "\n",
    "    def sanitise(self, mps_tweets_df):\n",
    "        # Sanitise author column\n",
    "        mps_tweets_df['author'] = mps_tweets_df['author'].apply(lambda x: x.lower())\n",
    "        mps_tweets_df['author'] = mps_tweets_df['author'].apply(lambda x: x.split()[0])\n",
    "        author_contents_mps_tweets_df = mps_tweets_df[['author', 'contents']].copy()\n",
    "\n",
    "        def extract_hash_tags(s):\n",
    "            return set([re.sub(r\"#+\", \"#\", k) for k in set(\n",
    "                [re.sub(r\"(\\W+)$\", \"\", j, flags=re.UNICODE) for j in\n",
    "                 set([i for i in s.split() if i.startswith(\"#\")])])])\n",
    "\n",
    "        dict_of_mps = {k: v for k, v in author_contents_mps_tweets_df.groupby('author')['contents']}\n",
    "        new_dict_of_mps = {}\n",
    "        for k, v in dict_of_mps.items():\n",
    "            vals = []\n",
    "            for val in v:\n",
    "                vals.append(extract_hash_tags(val))\n",
    "            new_dict_of_mps[k] = vals\n",
    "\n",
    "        for k in new_dict_of_mps.keys():\n",
    "            new_dict_of_mps[k] = [item for sublist in new_dict_of_mps[k] for item in sublist]\n",
    "\n",
    "        return new_dict_of_mps\n",
    "\n",
    "    def extract_features(self, all_tweets):\n",
    "        all_hashtags = []\n",
    "        count = 0\n",
    "        for dict_of_tweets in all_tweets[:-1]:\n",
    "            all_hashtags.append(self.sanitise(pd.DataFrame.from_dict(dict_of_tweets['posts'])))\n",
    "        final_hashtags_dict = {}\n",
    "        for dict_of_hashtags in all_hashtags:\n",
    "            for k, v in dict_of_hashtags.items():\n",
    "                if k in final_hashtags_dict.keys():\n",
    "                    val = final_hashtags_dict[k]\n",
    "                    if val != None:\n",
    "                        val = val + v\n",
    "                    else:\n",
    "                        val = v\n",
    "                    final_hashtags_dict[k] = val\n",
    "                else:\n",
    "                    final_hashtags_dict[k] = v\n",
    "        for k in final_hashtags_dict.keys():\n",
    "            final_hashtags_dict[k] = dict(Counter(final_hashtags_dict[k]))\n",
    "        return final_hashtags_dict\n",
    "\n",
    "    def extract_feature_names(self, X):\n",
    "        used_hashtags = []\n",
    "        for hashtag_list in X.values():\n",
    "            used_hashtags += list(hashtag_list.keys())\n",
    "        return list(set(used_hashtags))\n",
    "\n",
    "    def extract_mp_handles(self, X):\n",
    "        return list(X.keys())\n",
    "\n",
    "    def extract_useful_columns(self, X):\n",
    "        useful_columns = []\n",
    "        for i in range(0, X.shape[1]):\n",
    "            result = np.where(X[:, i] > 0)\n",
    "            if 5 < len(result[0]) < 500:\n",
    "                useful_columns.append(i)\n",
    "        return useful_columns\n",
    "    \n",
    "    def transform_y(self, X, y):\n",
    "        handles = list(y.keys())\n",
    "        for key in handles:\n",
    "            if key not in X.keys():\n",
    "                del y[key]\n",
    "        return y\n",
    "\n",
    "    # Return self nothing else to do here\n",
    "    def fit(self, X, y=None):\n",
    "        features = self.extract_features(X)\n",
    "        self._feature_names = self.extract_feature_names(features)\n",
    "#         features = self.mp_deleter(features, y)\n",
    "        feature_vector = self.vectoriser(features)\n",
    "        useful_columns = self.extract_useful_columns(feature_vector.toarray())\n",
    "        self._useful_hashtags = [self._feature_names[i] for i in useful_columns]\n",
    "        return self\n",
    "\n",
    "        # Method that describes what we need this transformer to do\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        features = self.extract_features(X)\n",
    "        if y is not None:\n",
    "            features = self.mp_deleter(features, y)\n",
    "        features = self.features_transformer(features)\n",
    "        if y is not None:\n",
    "            y = self.transform_y(features, y)\n",
    "        feature_vector = self.vectoriser(features)\n",
    "#         return feature_vector, y\n",
    "        return TfidfTransformer().fit_transform(feature_vector).toarray(), y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = joblib.load('./data/mps_2017_tweets.pckle')\n",
    "y = joblib.load('./data/2017_health_vote_labels.y')\n",
    "real_y = list(y.values())\n",
    "future_x = joblib.load('./data/mps_2019_tweets.pckle')\n",
    "future_y = joblib.load('./data/2019_health_vote_labels.y')\n",
    "real_future_y = list(future_y.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = TfidfFeatureExtractor()\n",
    "transformer = transformer.fit(x, y)\n",
    "x, real_y = transformer.transform(x, y)\n",
    "future_x, future_y = transformer.transform(future_x, future_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LogisticRegression(C=1)\n",
    "reg.fit(x, real_y)\n",
    "predictions = reg.predict(x)\n",
    "print(classification_report(np.array(list(future_y.values())), predictions, labels=[0,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Transformer that extracts columns passed as argument to its constructor\n",
    "class TimeFeatureExtractor(BaseEstimator, TransformerMixin):\n",
    "    # Class Constructor\n",
    "    def __init__(self):\n",
    "        self._feature_names = None\n",
    "        self._useful_hashtags = None\n",
    "\n",
    "    def vectoriser(self, features):\n",
    "        dict_vectorizer = DictVectorizer()\n",
    "        hashtags = list(features.values())\n",
    "        sparse_adjacency_matrix = dict_vectorizer.fit_transform(hashtags)\n",
    "        hashtags_df = pd.DataFrame(sparse_adjacency_matrix.todense(), index=features.keys()\n",
    "                                   , columns=dict_vectorizer.get_feature_names())\n",
    "        sparse_csr_matrix = sparse.csr_matrix(hashtags_df.values)\n",
    "        return sparse_csr_matrix\n",
    "    \n",
    "    def features_transformer(self, X):\n",
    "        new_features = collections.OrderedDict()\n",
    "        features = X\n",
    "        for mp_handle, feature_dict in features.items():\n",
    "            new_feature_dict = {}\n",
    "            for feature in self._useful_hashtags:\n",
    "                if feature in feature_dict:\n",
    "                    new_feature_dict[feature] = feature_dict[feature]\n",
    "                else:\n",
    "                    new_feature_dict[feature] = 0\n",
    "            new_features[mp_handle] = new_feature_dict\n",
    "        return new_features\n",
    "\n",
    "    def hashtag_deleter(self, X):\n",
    "        new_features = collections.OrderedDict()\n",
    "        features = X\n",
    "        for mp_handle, feature_dict in features.items():\n",
    "            new_feature_dict = {}\n",
    "            for feature in feature_dict:\n",
    "                if feature in self._useful_hashtags:\n",
    "                    new_feature_dict[feature] = feature_dict[feature]\n",
    "            new_features[mp_handle] = new_feature_dict\n",
    "        return new_features\n",
    "\n",
    "    def mp_deleter(self, X, y):\n",
    "        features = X\n",
    "        handles = list(features.keys())\n",
    "        for mp_handle in handles:\n",
    "            if mp_handle not in y.keys():\n",
    "                del features[mp_handle]\n",
    "        return features\n",
    "\n",
    "    def sanitise(self, mps_tweets_df):\n",
    "        # Sanitise author column\n",
    "        mps_tweets_df['author'] = mps_tweets_df['author'].apply(lambda x: x.lower())\n",
    "        mps_tweets_df['author'] = mps_tweets_df['author'].apply(lambda x: x.split()[0])\n",
    "        author_contents_mps_tweets_df = mps_tweets_df[['author', 'contents']].copy()\n",
    "\n",
    "        def extract_hash_tags(s):\n",
    "            return set([re.sub(r\"#+\", \"#\", k) for k in set(\n",
    "                [re.sub(r\"(\\W+)$\", \"\", j, flags=re.UNICODE) for j in\n",
    "                 set([i for i in s.split() if i.startswith(\"#\")])])])\n",
    "\n",
    "        dict_of_mps = {k: v for k, v in author_contents_mps_tweets_df.groupby('author')['contents']}\n",
    "        new_dict_of_mps = {}\n",
    "        for k, v in dict_of_mps.items():\n",
    "            vals = []\n",
    "            for val in v:\n",
    "                vals.append(extract_hash_tags(val))\n",
    "            new_dict_of_mps[k] = vals\n",
    "\n",
    "        for k in new_dict_of_mps.keys():\n",
    "            new_dict_of_mps[k] = [item for sublist in new_dict_of_mps[k] for item in sublist]\n",
    "\n",
    "        return new_dict_of_mps\n",
    "\n",
    "    def extract_features(self, all_tweets):\n",
    "        all_hashtags = []\n",
    "        count = 0\n",
    "        for dict_of_tweets in all_tweets[:-1]:\n",
    "            all_hashtags.append(self.sanitise(pd.DataFrame.from_dict(dict_of_tweets['posts'])))\n",
    "        final_hashtags_dict = {}\n",
    "        for dict_of_hashtags in all_hashtags:\n",
    "            for k, v in dict_of_hashtags.items():\n",
    "                if k in final_hashtags_dict.keys():\n",
    "                    val = final_hashtags_dict[k]\n",
    "                    if val != None:\n",
    "                        val = val + v\n",
    "                    else:\n",
    "                        val = v\n",
    "                    final_hashtags_dict[k] = val\n",
    "                else:\n",
    "                    final_hashtags_dict[k] = v\n",
    "        for k in final_hashtags_dict.keys():\n",
    "            final_hashtags_dict[k] = dict(Counter(final_hashtags_dict[k]))\n",
    "        return final_hashtags_dict\n",
    "\n",
    "    def extract_feature_names(self, X):\n",
    "        used_hashtags = []\n",
    "        for hashtag_list in X.values():\n",
    "            used_hashtags += list(hashtag_list.keys())\n",
    "        return list(set(used_hashtags))\n",
    "\n",
    "    def extract_mp_handles(self, X):\n",
    "        return list(X.keys())\n",
    "\n",
    "    def extract_useful_columns(self, X):\n",
    "        useful_columns = []\n",
    "        for i in range(0, X.shape[1]):\n",
    "            result = np.where(X[:, i] > 0)\n",
    "            if 5 < len(result[0]) < 500:\n",
    "                useful_columns.append(i)\n",
    "        return useful_columns\n",
    "    \n",
    "    def transform_y(self, X, y):\n",
    "        handles = list(y.keys())\n",
    "        for key in handles:\n",
    "            if key not in X.keys():\n",
    "                del y[key]\n",
    "        return y\n",
    "\n",
    "    # Return self nothing else to do here\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "        # Method that describes what we need this transformer to do\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        if y is not None:\n",
    "            features = self.mp_deleter(X, y)\n",
    "        if y is not None:\n",
    "            y = self.transform_y(features, y)\n",
    "        return feature_vector, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = joblib.load('./2017vectors100.x')\n",
    "y = joblib.load('./all_trimmed_mp_2017_brexit_vote_label.y')\n",
    "real_y = list(y.values())\n",
    "future_x = joblib.load('./2019vectors100.x')\n",
    "future_y = joblib.load('./all_new_2019_no_confidence_vote_labels.y')\n",
    "real_future_y = list(future_y.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = TimeFeatureExtractor()\n",
    "transformer = transformer.fit(x, y)\n",
    "x, real_y = transformer.transform(x, y)\n",
    "future_x, future_y = transformer.transform(future_x, future_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LogisticRegression(C=1)\n",
    "reg.fit(x, real_y)\n",
    "predictions = reg.predict(x)\n",
    "print(classification_report(np.array(list(future_y.values())), predictions, labels=[0,1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
